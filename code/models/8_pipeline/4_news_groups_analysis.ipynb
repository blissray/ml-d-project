{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "news = fetch_20newsgroups(subset='all')\n",
    "news.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rec.sport.hockey'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.target_names[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(news.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 뉴스 분류기 모델 만들기\n",
    "\n",
    "* 데이터 파악\n",
    "* 전처리(Preprocessing)\n",
    "\n",
    "    * 필요없는 단어 제거 (Data Cleansing)\n",
    "    * CountVectorizer & Tf-idfVectorizer\n",
    "\n",
    "---\n",
    "    \n",
    "* Modeling : BernoulliNB, MultinomialNB 사용\n",
    "  * Cross Validation(Kfold 이용)\n",
    "  \n",
    "---\n",
    "\n",
    "* Pipeline 이용\n",
    "\n",
    "---\n",
    "\n",
    "* Assignment Description\n",
    "     * 위 신문 데이터를 바탕으로 신문 내용별 분류기를 개발하라\n",
    "     * 위 데이터를 Traing / Test Dataset으로 나눠서 5-fold cross validation(5번 데이터를 training / testset으로 나눔, KV 활용)\n",
    "     * Naive Bayesian Classifier와 Count Vector를 활용하여 각각 성능을 테스트하라\n",
    "         * NB는 multinomial과 bernuoil 분포를 모두 사용하라\n",
    "     * 가능할 경우, TF-IDF vector를 활용해 볼것 (검색어 - tf-idf scikit-learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "   * 18846개의 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame({'News' : news.data, 'Target' : news.target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: Mamatha Devineni Ratnam &lt;mr47+@andrew.cm...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: mblawson@midway.ecn.uoknor.edu (Matthew ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: guyd@austin.ibm.com (Guy Dawson)\\nSubjec...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: Alexander Samuel McDiarmid &lt;am2o+@andrew...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                News  Target\n",
       "0  From: Mamatha Devineni Ratnam <mr47+@andrew.cm...      10\n",
       "1  From: mblawson@midway.ecn.uoknor.edu (Matthew ...       3\n",
       "2  From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject...      17\n",
       "3  From: guyd@austin.ibm.com (Guy Dawson)\\nSubjec...       3\n",
       "4  From: Alexander Samuel McDiarmid <am2o+@andrew...       4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From Mamatha Devineni Ratnam Subject Pens fans...</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From Matthew B Lawson Subject Which high perfo...</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From hilmi Hilmi Eren Subject Re ARMENIA SAYS ...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From Guy Dawson Subject Re IDE vs SCSI DMA and...</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From Alexander Samuel McDiarmid Subject driver...</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                News                    Target\n",
       "0  From Mamatha Devineni Ratnam Subject Pens fans...          rec.sport.hockey\n",
       "1  From Matthew B Lawson Subject Which high perfo...  comp.sys.ibm.pc.hardware\n",
       "2  From hilmi Hilmi Eren Subject Re ARMENIA SAYS ...     talk.politics.mideast\n",
       "3  From Guy Dawson Subject Re IDE vs SCSI DMA and...  comp.sys.ibm.pc.hardware\n",
       "4  From Alexander Samuel McDiarmid Subject driver...     comp.sys.mac.hardware"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target 데이터 -> 문자 라벨링(뉴스마다 어떤 뉴스인지 보기 편하도록 만들기 위해서)\n",
    "def word_labeling(lst, df):\n",
    "    for idx, name in enumerate(lst):\n",
    "        target_data = df['Target']\n",
    "        for idx_, num_label in enumerate(target_data):\n",
    "            if num_label == idx:\n",
    "                df.loc[idx_, 'Target'] = name\n",
    "    return df\n",
    "news_df = word_labeling(news['target_names'], news_df)\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data Cleansing\n",
    "    * 이메일 제거\n",
    "    * 불필요 숫자 제거\n",
    "    * 문자 아닌 특수문자 제거\n",
    "    * 단어 사이 공백 제거 : 띄어쓰기 별로 split해주고 join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleansing(df):\n",
    "    delete_email = re.sub(r'\\b[\\w\\+]+@[\\w]+.[\\w]+.[\\w]+.[\\w]+\\b', ' ', df)\n",
    "    delete_number = re.sub(r'\\b|\\d+|\\b', ' ',delete_email)\n",
    "    delete_non_word = re.sub(r'\\b[\\W]+\\b', ' ', delete_number)\n",
    "    cleaning_result = ' '.join(delete_non_word.split())\n",
    "    return cleaning_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From Mamatha Devineni Ratnam Subject Pens fans...</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From Matthew B Lawson Subject Which high perfo...</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From hilmi Hilmi Eren Subject Re ARMENIA SAYS ...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From Guy Dawson Subject Re IDE vs SCSI DMA and...</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From Alexander Samuel McDiarmid Subject driver...</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                News                    Target\n",
       "0  From Mamatha Devineni Ratnam Subject Pens fans...          rec.sport.hockey\n",
       "1  From Matthew B Lawson Subject Which high perfo...  comp.sys.ibm.pc.hardware\n",
       "2  From hilmi Hilmi Eren Subject Re ARMENIA SAYS ...     talk.politics.mideast\n",
       "3  From Guy Dawson Subject Re IDE vs SCSI DMA and...  comp.sys.ibm.pc.hardware\n",
       "4  From Alexander Samuel McDiarmid Subject driver...     comp.sys.mac.hardware"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.loc[:, 'News'] = news_df['News'].apply(data_cleansing)\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizer\n",
    "* CountVectorizer \n",
    "  * 문서 집합으로부터 단어의 수를 세어 카운트 행렬을 만듦\n",
    "* TfidfVectorizer \n",
    "    * 단어를 갯수 그대로 카운트하지 않고 모든 문서에 공통적으로 들어있는 단어의 경우 문서 구별 능력이 떨어진다고 보아 가중치를 축소하는 방법\n",
    "    * TF(Term Frequency) : 문서에서 해당 단어가 얼마나 나왔는지 나타내주는 빈도 수\n",
    "    * DF(Document Frequency) : 해당 단어가 있는 문서의 수\n",
    "    * IDF(Inverse Document Frequency) 해당 단어가 있는 문서의 수가 높아질 수록 가중치를 축소해주기 위해 역수 취해줌\n",
    "        * log(N / (1 + DF))      \n",
    "            * N : 전체 문서의 수\n",
    "    * TF-IDF = TF * IDF\n",
    "* CustomizedVectorizer - StemmedCounterVectorizer, StemmedTfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 1.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /home/sc82choi/miniconda3/envs/dsme/lib/python3.6/site-packages (from nltk) (1.12.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449909 sha256=6c7a28a069c7d0548d845b141e33324e3d2ba3238b1b56032f379a71858201ed\n",
      "  Stored in directory: /home/sc82choi/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.4.5\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['look', 'look', 'look']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import stem\n",
    "stmmer = stem.SnowballStemmer(\"english\")\n",
    "sentence = 'looking looks looked'\n",
    "[stmmer.stem(word) for word in sentence.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('imag', 'imag', 'imagin')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stmmer.stem(\"images\"), stmmer.stem(\"imaging\"), stmmer.stem(\"imagination\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sc82choi/miniconda3/envs/dsme/bin/pip\r\n"
     ]
    }
   ],
   "source": [
    "!which pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import  nltk\n",
    "enlish_stemmer = nltk.stem.SnowballStemmer(\"english\")\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer,self).build_analyzer()\n",
    "        return lambda doc: (enlish_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'look': 0}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StemmedCountVectorizer(min_df=1, stop_words=\"english\").fit([sentence]).vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'looking': 1, 'looks': 2, 'looked': 0}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVectorizer(min_df=1, stop_words=\"english\").fit([sentence]).vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "enlish_stemmer = nltk.stem.SnowballStemmer(\"english\")\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedTfidfVectorizer,self).build_analyzer()\n",
    "        return lambda doc: (enlish_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "* Pipeline\n",
    "* Gridsearch\n",
    "* Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'is', 'a')\n",
      "('is', 'a', 'foo')\n",
      "('a', 'foo', 'bar')\n",
      "('foo', 'bar', 'sentences')\n",
      "('bar', 'sentences', 'and')\n",
      "('sentences', 'and', 'i')\n",
      "('and', 'i', 'want')\n",
      "('i', 'want', 'to')\n",
      "('want', 'to', 'ngramize')\n",
      "('to', 'ngramize', 'it')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "sentence = 'this is a foo bar sentences and i want to ngramize it'\n",
    "n = 6\n",
    "sixgrams = ngrams(sentence.split(), 3)\n",
    "for grams in sixgrams:\n",
    "  print (grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "class DenseTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB,GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "vectorizer = [CountVectorizer(), TfidfVectorizer(), \n",
    "              StemmedCountVectorizer(), StemmedTfidfVectorizer()]\n",
    "# algorithms = [BernoulliNB(), MultinomialNB(), GaussianNB(), LogisticRegression()]\n",
    "\n",
    "algorithms = [MultinomialNB(), LogisticRegression()]\n",
    "\n",
    "\n",
    "pipelines  = [] \n",
    "import itertools\n",
    "for case in list(itertools.product(vectorizer, algorithms)):\n",
    "    if isinstance(case[1], GaussianNB):\n",
    "        case = list(case)\n",
    "        case.insert(1,  DenseTransformer())\n",
    "    pipelines.append(make_pipeline(*case))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Pipeline(memory=None,\n",
       "          steps=[('countvectorizer',\n",
       "                  CountVectorizer(analyzer='word', binary=False,\n",
       "                                  decode_error='strict',\n",
       "                                  dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                  input='content', lowercase=True, max_df=1.0,\n",
       "                                  max_features=None, min_df=1,\n",
       "                                  ngram_range=(1, 1), preprocessor=None,\n",
       "                                  stop_words=None, strip_accents=None,\n",
       "                                  token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                  tokenizer=None, vocabulary=None)),\n",
       "                 ('multinomialnb',\n",
       "                  MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "          verbose=False), Pipeline(memory=None,\n",
       "          steps=[('countvectorizer',\n",
       "                  CountVectorizer(analyzer='word', binary=False,\n",
       "                                  decode_error='strict',\n",
       "                                  dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                  input='content', lowercase=True, max_df=1.0,\n",
       "                                  max_features=None, min_df=1,\n",
       "                                  ngram_range=(1, 1), preprocessor=None,\n",
       "                                  stop_words=None, strip_accents=None,\n",
       "                                  token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                  tokenizer=None, vocabulary=None)),\n",
       "                 ('logisticregression',\n",
       "                  LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                     fit_intercept=True, intercept_scaling=1,\n",
       "                                     l1_ratio=None, max_iter=100,\n",
       "                                     multi_class='warn', n_jobs=None,\n",
       "                                     penalty='l2', random_state=None,\n",
       "                                     solver='warn', tol=0.0001, verbose=0,\n",
       "                                     warm_start=False))],\n",
       "          verbose=False), Pipeline(memory=None,\n",
       "          steps=[('tfidfvectorizer',\n",
       "                  TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                  decode_error='strict',\n",
       "                                  dtype=<class 'numpy.float64'>,\n",
       "                                  encoding='utf-8', input='content',\n",
       "                                  lowercase=True, max_df=1.0, max_features=None,\n",
       "                                  min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                  preprocessor=None, smooth_idf=True,\n",
       "                                  stop_words=None, strip_accents=None,\n",
       "                                  sublinear_tf=False,\n",
       "                                  token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                  tokenizer=None, use_idf=True,\n",
       "                                  vocabulary=None)),\n",
       "                 ('multinomialnb',\n",
       "                  MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "          verbose=False), Pipeline(memory=None,\n",
       "          steps=[('tfidfvectorizer',\n",
       "                  TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                  decode_error='strict',\n",
       "                                  dtype=<class 'numpy.float64'>,\n",
       "                                  encoding='utf-8', input='content',\n",
       "                                  lowercase=True, max_df=1.0, max_features=None,\n",
       "                                  min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                  preprocessor=None, smooth_idf=True,\n",
       "                                  stop_words=None, strip_accents=None,\n",
       "                                  sublinear_tf=False,\n",
       "                                  token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                  tokenizer=None, use_idf=True,\n",
       "                                  vocabulary=None)),\n",
       "                 ('logisticregression',\n",
       "                  LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                     fit_intercept=True, intercept_scaling=1,\n",
       "                                     l1_ratio=None, max_iter=100,\n",
       "                                     multi_class='warn', n_jobs=None,\n",
       "                                     penalty='l2', random_state=None,\n",
       "                                     solver='warn', tol=0.0001, verbose=0,\n",
       "                                     warm_start=False))],\n",
       "          verbose=False), Pipeline(memory=None,\n",
       "          steps=[('stemmedcountvectorizer',\n",
       "                  StemmedCountVectorizer(analyzer='word', binary=False,\n",
       "                                         decode_error='strict',\n",
       "                                         dtype=<class 'numpy.int64'>,\n",
       "                                         encoding='utf-8', input='content',\n",
       "                                         lowercase=True, max_df=1.0,\n",
       "                                         max_features=None, min_df=1,\n",
       "                                         ngram_range=(1, 1), preprocessor=None,\n",
       "                                         stop_words=None, strip_accents=None,\n",
       "                                         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                         tokenizer=None, vocabulary=None)),\n",
       "                 ('multinomialnb',\n",
       "                  MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "          verbose=False), Pipeline(memory=None,\n",
       "          steps=[('stemmedcountvectorizer',\n",
       "                  StemmedCountVectorizer(analyzer='word', binary=False,\n",
       "                                         decode_error='strict',\n",
       "                                         dtype=<class 'numpy.int64'>,\n",
       "                                         encoding='utf-8', input='content',\n",
       "                                         lowercase=True, max_df=1.0,\n",
       "                                         max_features=None, min_df=1,\n",
       "                                         ngram_range=(1, 1), preprocessor=None,\n",
       "                                         stop_words=None, strip_accents=None,\n",
       "                                         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                         tokenizer=None, vocabulary=None)),\n",
       "                 ('logisticregression',\n",
       "                  LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                     fit_intercept=True, intercept_scaling=1,\n",
       "                                     l1_ratio=None, max_iter=100,\n",
       "                                     multi_class='warn', n_jobs=None,\n",
       "                                     penalty='l2', random_state=None,\n",
       "                                     solver='warn', tol=0.0001, verbose=0,\n",
       "                                     warm_start=False))],\n",
       "          verbose=False), Pipeline(memory=None,\n",
       "          steps=[('stemmedtfidfvectorizer',\n",
       "                  StemmedTfidfVectorizer(analyzer='word', binary=False,\n",
       "                                         decode_error='strict',\n",
       "                                         dtype=<class 'numpy.float64'>,\n",
       "                                         encoding='utf-8', input='content',\n",
       "                                         lowercase=True, max_df=1.0,\n",
       "                                         max_features=None, min_df=1,\n",
       "                                         ngram_range=(1, 1), norm='l2',\n",
       "                                         preprocessor=None, smooth_idf=True,\n",
       "                                         stop_words=None, strip_accents=None,\n",
       "                                         sublinear_tf=False,\n",
       "                                         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                         tokenizer=None, use_idf=True,\n",
       "                                         vocabulary=None)),\n",
       "                 ('multinomialnb',\n",
       "                  MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "          verbose=False), Pipeline(memory=None,\n",
       "          steps=[('stemmedtfidfvectorizer',\n",
       "                  StemmedTfidfVectorizer(analyzer='word', binary=False,\n",
       "                                         decode_error='strict',\n",
       "                                         dtype=<class 'numpy.float64'>,\n",
       "                                         encoding='utf-8', input='content',\n",
       "                                         lowercase=True, max_df=1.0,\n",
       "                                         max_features=None, min_df=1,\n",
       "                                         ngram_range=(1, 1), norm='l2',\n",
       "                                         preprocessor=None, smooth_idf=True,\n",
       "                                         stop_words=None, strip_accents=None,\n",
       "                                         sublinear_...\n",
       "                                         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                         tokenizer=None, use_idf=True,\n",
       "                                         vocabulary=None)),\n",
       "                 ('logisticregression',\n",
       "                  LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                     fit_intercept=True, intercept_scaling=1,\n",
       "                                     l1_ratio=None, max_iter=100,\n",
       "                                     multi_class='warn', n_jobs=None,\n",
       "                                     penalty='l2', random_state=None,\n",
       "                                     solver='warn', tol=0.0001, verbose=0,\n",
       "                                     warm_start=False))],\n",
       "          verbose=False)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 'A'),\n",
       " ('1', 'B'),\n",
       " ('1', 'C'),\n",
       " ('2', 'A'),\n",
       " ('2', 'B'),\n",
       " ('2', 'C'),\n",
       " ('3', 'A'),\n",
       " ('3', 'B'),\n",
       " ('3', 'C')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "list(itertools.product([\"1\", \"2\",\"3\"], [\"A\", \"B\",\"C\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer Common params\n",
    "ngrams_params = [(1,1),(1,3)]\n",
    "stopword_params = [\"english\"]\n",
    "lowercase_params = [True, False]\n",
    "max_df_params = np.linspace(0.4, 0.6, num=6)\n",
    "min_df_params = np.linspace(0.0, 0.2, num=6)\n",
    "\n",
    "attributes = {\"ngram_range\":ngrams_params, \"max_df\":max_df_params,\"min_df\":min_df_params,\n",
    "              \"lowercase\":lowercase_params,\"stop_words\":stopword_params}\n",
    "vectorizer_names = [\"countvectorizer\",\"tfidfvectorizer\",\n",
    "                    \"stemmedcountvectorizer\",\"stemmedtfidfvectorizer\"]\n",
    "vectorizer_params_dict = {}\n",
    "\n",
    "for vect_name in vectorizer_names:\n",
    "    vectorizer_params_dict[vect_name] = {}\n",
    "    for key, value in attributes.items():\n",
    "        param_name = vect_name + \"__\" + key\n",
    "        vectorizer_params_dict[vect_name][param_name] =  value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'countvectorizer': {'countvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "  'countvectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
       "  'countvectorizer__min_df': array([0.  , 0.04, 0.08, 0.12, 0.16, 0.2 ]),\n",
       "  'countvectorizer__lowercase': [True, False],\n",
       "  'countvectorizer__stop_words': ['english']},\n",
       " 'tfidfvectorizer': {'tfidfvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "  'tfidfvectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
       "  'tfidfvectorizer__min_df': array([0.  , 0.04, 0.08, 0.12, 0.16, 0.2 ]),\n",
       "  'tfidfvectorizer__lowercase': [True, False],\n",
       "  'tfidfvectorizer__stop_words': ['english']},\n",
       " 'stemmedcountvectorizer': {'stemmedcountvectorizer__ngram_range': [(1, 1),\n",
       "   (1, 3)],\n",
       "  'stemmedcountvectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
       "  'stemmedcountvectorizer__min_df': array([0.  , 0.04, 0.08, 0.12, 0.16, 0.2 ]),\n",
       "  'stemmedcountvectorizer__lowercase': [True, False],\n",
       "  'stemmedcountvectorizer__stop_words': ['english']},\n",
       " 'stemmedtfidfvectorizer': {'stemmedtfidfvectorizer__ngram_range': [(1, 1),\n",
       "   (1, 3)],\n",
       "  'stemmedtfidfvectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
       "  'stemmedtfidfvectorizer__min_df': array([0.  , 0.04, 0.08, 0.12, 0.16, 0.2 ]),\n",
       "  'stemmedtfidfvectorizer__lowercase': [True, False],\n",
       "  'stemmedtfidfvectorizer__stop_words': ['english']}}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'multinomialnb': {'multinomialnb__alpha': array([1.])},\n",
       " 'logisticregression': [{'logisticregression__multi_class': ['multinomial'],\n",
       "   'logisticregression__solver': ['saga'],\n",
       "   'logisticregression__penalty': ['l1'],\n",
       "   'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]},\n",
       "  {'logisticregression__multi_class': ['ovr'],\n",
       "   'logisticregression__solver': ['liblinear'],\n",
       "   'logisticregression__penalty': ['l2'],\n",
       "   'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]}]}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Algorithms parameters\n",
    "algorithm_names = [\"bernoullinb\",\"multinomialnb\",\"gaussiannb\",\"logisticregression\"]\n",
    "algorithm_names = [\"multinomialnb\", \"logisticregression\"]\n",
    "\n",
    "algorithm_params_dict = {}\n",
    "\n",
    "\n",
    "#'bernoullinb', BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True))])\n",
    "alpha_params = np.linspace(1.0, 1.0, num=1)\n",
    "for i in range(1):\n",
    "    algorithm_params_dict[algorithm_names[i]] = {\n",
    "    algorithm_names[i]+ \"__alpha\" : alpha_params    \n",
    "    }\n",
    "# algorithm_params_dict[algorithm_names[2]] = {}\n",
    "\n",
    "\n",
    "# LogisticRegression    \n",
    "# multi_class : str, {‘ovr’, ‘multinomial’}, default: ‘ovr’\n",
    "# C : float, default: 1.0\n",
    "# solver : {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’},\n",
    "# n_jobs : int, default: 1\n",
    "# penalty : str, ‘l1’ or ‘l2’, default: ‘l2’\n",
    "\n",
    "# multi_class_params = [\"ovr\", \"multinomial\"]\n",
    "c_params = [0.1,  5.0, 7.0, 10.0, 15.0, 20.0, 100.0]\n",
    "\n",
    "\n",
    "\n",
    "algorithm_params_dict[algorithm_names[1]] = [{\n",
    "    \"logisticregression__multi_class\" : [\"multinomial\"],\n",
    "    \"logisticregression__solver\" : [\"saga\"],\n",
    "    \"logisticregression__penalty\" : [\"l1\"],\n",
    "    \"logisticregression__C\" : c_params\n",
    "    },{\n",
    "    \"logisticregression__multi_class\" : [\"ovr\"],\n",
    "    \"logisticregression__solver\" : ['liblinear'],\n",
    "    \"logisticregression__penalty\" : [\"l2\"],\n",
    "    \"logisticregression__C\" : c_params\n",
    "    }\n",
    "    ]\n",
    "algorithm_params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'countvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "  'countvectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
       "  'countvectorizer__min_df': array([0.  , 0.04, 0.08, 0.12, 0.16, 0.2 ]),\n",
       "  'countvectorizer__lowercase': [True, False],\n",
       "  'countvectorizer__stop_words': ['english'],\n",
       "  'multinomialnb__alpha': array([1.])},\n",
       " [{'countvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "   'countvectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
       "   'countvectorizer__min_df': array([0.  , 0.04, 0.08, 0.12, 0.16, 0.2 ]),\n",
       "   'countvectorizer__lowercase': [True, False],\n",
       "   'countvectorizer__stop_words': ['english'],\n",
       "   'logisticregression__multi_class': ['ovr'],\n",
       "   'logisticregression__solver': ['liblinear'],\n",
       "   'logisticregression__penalty': ['l2'],\n",
       "   'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]},\n",
       "  {'countvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "   'countvectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
       "   'countvectorizer__min_df': array([0.  , 0.04, 0.08, 0.12, 0.16, 0.2 ]),\n",
       "   'countvectorizer__lowercase': [True, False],\n",
       "   'countvectorizer__stop_words': ['english'],\n",
       "   'logisticregression__multi_class': ['ovr'],\n",
       "   'logisticregression__solver': ['liblinear'],\n",
       "   'logisticregression__penalty': ['l2'],\n",
       "   'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]}],\n",
       " {'tfidfvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "  'tfidfvectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
       "  'tfidfvectorizer__min_df': array([0.  , 0.04, 0.08, 0.12, 0.16, 0.2 ]),\n",
       "  'tfidfvectorizer__lowercase': [True, False],\n",
       "  'tfidfvectorizer__stop_words': ['english'],\n",
       "  'multinomialnb__alpha': array([1.])},\n",
       " [{'tfidfvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "   'tfidfvectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
       "   'tfidfvectorizer__min_df': array([0.  , 0.04, 0.08, 0.12, 0.16, 0.2 ]),\n",
       "   'tfidfvectorizer__lowercase': [True, False],\n",
       "   'tfidfvectorizer__stop_words': ['english'],\n",
       "   'logisticregression__multi_class': ['ovr'],\n",
       "   'logisticregression__solver': ['liblinear'],\n",
       "   'logisticregression__penalty': ['l2'],\n",
       "   'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]},\n",
       "  {'tfidfvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "   'tfidfvectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
       "   'tfidfvectorizer__min_df': array([0.  , 0.04, 0.08, 0.12, 0.16, 0.2 ]),\n",
       "   'tfidfvectorizer__lowercase': [True, False],\n",
       "   'tfidfvectorizer__stop_words': ['english'],\n",
       "   'logisticregression__multi_class': ['ovr'],\n",
       "   'logisticregression__solver': ['liblinear'],\n",
       "   'logisticregression__penalty': ['l2'],\n",
       "   'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]}],\n",
       " {'stemmedcountvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "  'stemmedcountvectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
       "  'stemmedcountvectorizer__min_df': array([0.  , 0.04, 0.08, 0.12, 0.16, 0.2 ]),\n",
       "  'stemmedcountvectorizer__lowercase': [True, False],\n",
       "  'stemmedcountvectorizer__stop_words': ['english'],\n",
       "  'multinomialnb__alpha': array([1.])},\n",
       " [{'stemmedcountvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "   'stemmedcountvectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
       "   'stemmedcountvectorizer__min_df': array([0.  , 0.04, 0.08, 0.12, 0.16, 0.2 ]),\n",
       "   'stemmedcountvectorizer__lowercase': [True, False],\n",
       "   'stemmedcountvectorizer__stop_words': ['english'],\n",
       "   'logisticregression__multi_class': ['ovr'],\n",
       "   'logisticregression__solver': ['liblinear'],\n",
       "   'logisticregression__penalty': ['l2'],\n",
       "   'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]},\n",
       "  {'stemmedcountvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "   'stemmedcountvectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
       "   'stemmedcountvectorizer__min_df': array([0.  , 0.04, 0.08, 0.12, 0.16, 0.2 ]),\n",
       "   'stemmedcountvectorizer__lowercase': [True, False],\n",
       "   'stemmedcountvectorizer__stop_words': ['english'],\n",
       "   'logisticregression__multi_class': ['ovr'],\n",
       "   'logisticregression__solver': ['liblinear'],\n",
       "   'logisticregression__penalty': ['l2'],\n",
       "   'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]}],\n",
       " {'stemmedtfidfvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "  'stemmedtfidfvectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
       "  'stemmedtfidfvectorizer__min_df': array([0.  , 0.04, 0.08, 0.12, 0.16, 0.2 ]),\n",
       "  'stemmedtfidfvectorizer__lowercase': [True, False],\n",
       "  'stemmedtfidfvectorizer__stop_words': ['english'],\n",
       "  'multinomialnb__alpha': array([1.])},\n",
       " [{'stemmedtfidfvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "   'stemmedtfidfvectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
       "   'stemmedtfidfvectorizer__min_df': array([0.  , 0.04, 0.08, 0.12, 0.16, 0.2 ]),\n",
       "   'stemmedtfidfvectorizer__lowercase': [True, False],\n",
       "   'stemmedtfidfvectorizer__stop_words': ['english'],\n",
       "   'logisticregression__multi_class': ['ovr'],\n",
       "   'logisticregression__solver': ['liblinear'],\n",
       "   'logisticregression__penalty': ['l2'],\n",
       "   'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]},\n",
       "  {'stemmedtfidfvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "   'stemmedtfidfvectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
       "   'stemmedtfidfvectorizer__min_df': array([0.  , 0.04, 0.08, 0.12, 0.16, 0.2 ]),\n",
       "   'stemmedtfidfvectorizer__lowercase': [True, False],\n",
       "   'stemmedtfidfvectorizer__stop_words': ['english'],\n",
       "   'logisticregression__multi_class': ['ovr'],\n",
       "   'logisticregression__solver': ['liblinear'],\n",
       "   'logisticregression__penalty': ['l2'],\n",
       "   'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]}]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_params= []\n",
    "for case in list(itertools.product(vectorizer_names, algorithm_names)):\n",
    "    vect_params = vectorizer_params_dict[case[0]].copy()\n",
    "    algo_params = algorithm_params_dict[case[1]]\n",
    "    \n",
    "    if isinstance(algo_params, dict):\n",
    "        vect_params.update(algo_params)\n",
    "        pipeline_params.append(vect_params)\n",
    "    else:\n",
    "        temp = []\n",
    "        for param in algo_params:\n",
    "            vect_params.update(param)\n",
    "            temp.append(vect_params)\n",
    "        pipeline_params.append(temp)\n",
    "pipeline_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn! Learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_data = news_df.loc[:, 'News'].tolist()\n",
    "y_data = news_df['Target'].tolist()\n",
    "y = LabelEncoder().fit_transform(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
      "             estimator=Pipeline(memory=None,\n",
      "                                steps=[('countvectorizer',\n",
      "                                        CountVectorizer(analyzer='word',\n",
      "                                                        binary=False,\n",
      "                                                        decode_error='strict',\n",
      "                                                        dtype=<class 'numpy.int64'>,\n",
      "                                                        encoding='utf-8',\n",
      "                                                        input='content',\n",
      "                                                        lowercase=True,\n",
      "                                                        max_df=1.0,\n",
      "                                                        max_features=None,\n",
      "                                                        min_df=1,\n",
      "                                                        ngram_range=(1, 1),\n",
      "                                                        preprocessor=None,\n",
      "                                                        stop_words=None,\n",
      "                                                        strip_accen...\n",
      "             param_grid={'countvectorizer__lowercase': [True, False],\n",
      "                         'countvectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
      "                         'countvectorizer__min_df': array([0.  , 0.04, 0.08, 0.12, 0.16, 0.2 ]),\n",
      "                         'countvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
      "                         'countvectorizer__stop_words': ['english'],\n",
      "                         'multinomialnb__alpha': array([1.])},\n",
      "             pre_dispatch='2*n_jobs', refit='accuracy',\n",
      "             return_train_score=False, scoring=['accuracy'], verbose=1)\n",
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=36)]: Using backend LokyBackend with 36 concurrent workers.\n",
      "[Parallel(n_jobs=36)]: Done 128 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=36)]: Done 378 tasks      | elapsed: 12.2min\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "scoring = ['accuracy']\n",
    "estimator_results = []\n",
    "for i, (estimator, params) in enumerate(zip(pipelines,pipeline_params)):\n",
    "    n_jobs = 36\n",
    "#     if i+1 % 3 == 0:\n",
    "#         n_jobs = 2\n",
    "    gs_estimator = GridSearchCV(\n",
    "            refit=\"accuracy\", estimator=estimator,param_grid=params, scoring=scoring, cv=5, verbose=1, n_jobs=n_jobs)\n",
    "    print(gs_estimator)\n",
    "\n",
    "    gs_estimator.fit(X_data, y)\n",
    "    estimator_results.append(gs_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "result_df_dict = {}\n",
    "result_attributes = [\"vectorizer\", \"model\", \"accuracy\", \"recall_macro\",\"precision_macro\" , \"min_df\", \n",
    "                     \"lowercase\", \"max_df\", \"binarize\", \"alpha\", \"ngram_range\"\n",
    "                     \"multi_class\", \"penalty\", \"solver\", \"C\"]\n",
    "\n",
    "pieline_list =  list(itertools.product(vectorizer_names, algorithm_names))\n",
    "\n",
    "for att in result_attributes:\n",
    "    result_df_dict[att] = [None for i in range(16)]\n",
    "\n",
    "result_df = DataFrame(result_df_dict)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pieline_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-a728c3d21ff5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpieline_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pieline_list' is not defined"
     ]
    }
   ],
   "source": [
    "pieline_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, estiamtor in enumerate(estimator_results):\n",
    "    best_estimator = estiamtor.best_estimator_\n",
    "    best_index = estiamtor.best_index_\n",
    "    result_df_dict[\"vectorizer\"][i] = pieline_list[i][0]\n",
    "    result_df_dict[\"model\"][i] = pieline_list[i][1]\n",
    "    result_df_dict[\"accuracy\"][i] = estiamtor.best_score_\n",
    "#     result_df_dict[\"recall_micro\"][i] = estiamtor.cv_results_[\"mean_test_recall_micro\"][best_index]\n",
    "#     result_df_dict[\"precision_micro\"][i] = estiamtor.cv_results_[\"mean_test_precision_micro\"][best_index]\n",
    "    for key, value in estiamtor.best_params_.items():\n",
    "        if key.split(\"__\")[1] in result_df_dict:\n",
    "            name = key.split(\"__\")[1]\n",
    "            result_df_dict[key.split(\"__\")[1]][i] = value\n",
    "#     print(estiamtor.best_params_)\n",
    "#     print(a.named_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = DataFrame(result_df_dict, columns=result_attributes)\n",
    "result_df.sort_values(\"accuracy\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-bf7dab556292>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'result_df' is not defined"
     ]
    }
   ],
   "source": [
    "result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
